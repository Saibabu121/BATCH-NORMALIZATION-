Batch normalization is a technique to standardize the inputs to a network, applied to ether the activations of a prior layer or inputs directly.
Batch normalization accelerates training, in some cases by halving the epochs or better, and provides some regularization, reducing generalization error.
